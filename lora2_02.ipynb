{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "åˆ†è©å™¨å·²æˆåŠŸè¼‰å…¥ï¼\n"
     ]
    }
   ],
   "source": [
    "# cell 1 â€” åƒæ•¸ã€è·¯å¾‘èˆ‡åŸºæœ¬è¨­å®š\n",
    "\n",
    "import os, json, random, re, unicodedata\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, TrainingArguments, GenerationConfig,\n",
    "    AutoModelForCausalLM, AutoModel\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import torch.nn as nn\n",
    "\n",
    "# å›ºå®šç¨®å­\n",
    "torch.manual_seed(42); random.seed(42)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"MediaTek-Research/Llama-Breeze2-3B-Instruct-v0_1\"\n",
    "\n",
    "cot_train_data_file = r\"./0801/train_chatml_cot.jsonl\"\n",
    "ao_train_data_file  = r\"./0801/train_chatml.jsonl\"\n",
    "val_data_file       = r\"./0801/val_chatml.jsonl\"\n",
    "test_data_file      = r\"./0801/test_chatml.jsonl\"\n",
    "\n",
    "# è¼¸å‡º\n",
    "OUTPUT_DIR_COT = \"./breeze2-lora-cot-02\"\n",
    "OUTPUT_DIR_AO  = \"./breeze2-lora-ao-02\"\n",
    "os.makedirs(OUTPUT_DIR_COT, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_AO,  exist_ok=True)\n",
    "os.makedirs(\"results\",      exist_ok=True)\n",
    "\n",
    "# è¶…åƒ\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "num_train_epochs = 20\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.05\n",
    "warmup_ratio = 0.08\n",
    "max_grad_norm = 0.5\n",
    "logging_steps = 20\n",
    "eval_steps = 200\n",
    "save_steps = 200\n",
    "max_seq_length = 1024\n",
    "max_new_tokens = 1024  # è©•ä¼°/å­˜è¼¸å‡º\n",
    "\n",
    "# ç”Ÿæˆè¨­å®š\n",
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=False,         \n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    "    repetition_penalty=1.05,\n",
    ")\n",
    "\n",
    "# æç¤ºè©\n",
    "system_prompt_cot = (\n",
    "    \"ä½ æ˜¯ä¸€å€‹æ­·å²å­¸å®¶ï¼Œè«‹ä½ é‡å°å•é¡Œåœ¨è…¦ä¸­é€²è¡Œè©³ç´°çš„é‚è¼¯æ¨ç†ã€‚ä½ çš„è¼¸å‡ºå¿…é ˆåŒ…å«å®Œæ•´çš„æ¨ç†éç¨‹å’Œæœ€çµ‚ç­”æ¡ˆã€‚æ ¼å¼ç‚ºï¼šæ¨ç†éç¨‹ï¼š\\[ä½ çš„å®Œæ•´æ¨ç†éç¨‹\\]æœ€çµ‚ç­”æ¡ˆï¼š\\[ä½ çš„æœ€çµ‚çµè«–\\]\"\n",
    ")\n",
    "system_prompt_ao  = (\n",
    "    \"ä½ æ˜¯ä¸€å€‹æ­·å²å­¸å®¶ã€‚è«‹é‡å°å•é¡Œï¼Œç›´æ¥çµ¦å‡ºæœ€çµ‚ç­”æ¡ˆã€‚æ ¼å¼ç‚ºï¼šæœ€çµ‚ç­”æ¡ˆï¼š\\[ä½ çš„æœ€çµ‚çµè«–\\]\"\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.img_context_token_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f93609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: å–å¾—æ–‡å­— LLM = LlamaForCausalLM\n"
     ]
    }
   ],
   "source": [
    "# cell 2 â€” è¼‰å…¥ Breeze2 wrapper â†’ æŠ½å‡º language_model\n",
    "\n",
    "# æŒ‰æ¨¡å‹å¡ç¤ºä¾‹è¨­ç½® img_context_token_idï¼ˆå° wrapper å‹å–„ï¼‰\n",
    "IMG_CTX_ID = 128212\n",
    "\n",
    "# åªç”¨å®˜æ–¹ wrapper ä¾†è¼‰å…¥ï¼Œé¿å… AutoConfig å° 'internvl_chat' çš„ç›´è¼‰å•é¡Œ\n",
    "wrap = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=None,\n",
    "    img_context_token_id=IMG_CTX_ID,\n",
    ").eval().to(DEVICE)\n",
    "\n",
    "# æŠ½å‡ºç´”æ–‡å­— LLMï¼ˆé€™å€‹æ‰äº¤çµ¦ LoRA/TRLï¼‰\n",
    "if hasattr(wrap, \"language_model\"):\n",
    "    base_model = wrap.language_model\n",
    "else:\n",
    "    for cand in (\"llm\", \"text_model\", \"lm\", \"model\"):\n",
    "        if hasattr(wrap, cand):\n",
    "            base_model = getattr(wrap, cand)\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError(\"æ‰¾ä¸åˆ°æ–‡å­— LLM å­æ¨¡çµ„ï¼ˆlanguage_model/llm/text_model/lm/modelï¼‰ã€‚\")\n",
    "\n",
    "base_model.to(DEVICE).eval()\n",
    "print(\"OK: å–å¾—æ–‡å­— LLM =\", type(base_model).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7c190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3 â€” å°‡ ChatML å±•å¹³æˆç´”æ–‡å­—æ¨£æœ¬ï¼ˆdataset_text_field='text' ç”¨ï¼‰\n",
    "\n",
    "SYS_FALLBACK = \"è«‹åœ¨è…¦ä¸­é€æ­¥æ¨ç†ï¼Œä½†æœ€çµ‚åªè¼¸å‡ºä¸€è¡Œï¼šæœ€çµ‚ç­”æ¡ˆï¼šXXXX\"\n",
    "\n",
    "def formatting_prompts_func(batch):\n",
    "    outs = []\n",
    "    for msgs in batch[\"messages\"]:\n",
    "        m = msgs\n",
    "        if not m or m[0].get(\"role\") != \"system\":\n",
    "            m = [{\"role\":\"system\",\"content\": SYS_FALLBACK}] + m\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            m, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        outs.append(text)\n",
    "    return {\"text\": outs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4 â€” LoRA\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "def build_lora_model():\n",
    "    m = get_peft_model(base_model, lora_config)\n",
    "    m.print_trainable_parameters()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1573d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5 â€” Logging callback\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CustomLoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.logs = defaultdict(list)\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if getattr(state, \"is_local_process_zero\", True):\n",
    "            if logs is None: return\n",
    "            if \"loss\" in logs:\n",
    "                self.logs[\"train_loss\"].append(logs[\"loss\"])\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.logs[\"eval_loss\"].append(logs[\"eval_loss\"])\n",
    "\n",
    "def plot_losses(logs, title, output_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if logs['train_loss']:\n",
    "        plt.plot(logs['train_loss'], label='Training Loss')\n",
    "    if logs['eval_loss']:\n",
    "        gap = max(1, len(logs['train_loss']) // max(1, len(logs['eval_loss'])))\n",
    "        xs = list(range(0, gap*len(logs['eval_loss']), gap))\n",
    "        plt.plot(xs, logs['eval_loss'], 'o-', label='Validation Loss')\n",
    "    plt.title(title); plt.xlabel('Training Steps'); plt.ylabel('Loss')\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6 â€” å–®æ¬¡è¨“ç·´å‡½å¼ï¼ˆCoT / AO éƒ½ç”¨ï¼‰\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "def train_and_save_model(train_file, val_file, output_dir, model_type):\n",
    "    print(f\"\\n=== é–‹å§‹è¨“ç·´ {model_type} æ¨¡å‹ï¼ˆ_02ï¼‰ ===\")\n",
    "\n",
    "    # é‡æ–°æ›ä¸€å€‹æ–°çš„ LoRAï¼ˆé¿å…é‡è¤‡ç–Š adapterï¼‰\n",
    "    peft_model = build_lora_model()\n",
    "\n",
    "    # è®€è³‡æ–™ä¸¦è½‰ç‚ºç´”æ–‡å­—\n",
    "    train_ds = load_dataset(\"json\", data_files={\"train\": train_file}, split=\"train\")\n",
    "    val_ds = load_dataset(\"json\", data_files={\"validation\": val_file}, split=\"validation\")\n",
    "    train_text = train_ds.map(formatting_prompts_func, batched=True)\n",
    "    val_text = val_ds.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    cb = CustomLoggingCallback()\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        logging_steps=logging_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=peft_model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=True,\n",
    "        args=args,\n",
    "        train_dataset=train_text,\n",
    "        eval_dataset=val_text,\n",
    "        callbacks=[cb],\n",
    "    )\n",
    "\n",
    "    # åˆ¤æ–·æ˜¯å¦å¾æª¢æŸ¥é»æ¢å¾©\n",
    "    # æª¢æŸ¥ output_dir æ˜¯å¦å­˜åœ¨ï¼Œä¸”è£¡é¢æ˜¯å¦æœ‰ checkpoint æª”æ¡ˆ\n",
    "    checkpoint_dirs = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "    if len(checkpoint_dirs) > 0:\n",
    "        print(f\"åœ¨ {output_dir} æ‰¾åˆ°æª¢æŸ¥é»ï¼Œå¾ä¸Šä¸€æ¬¡è¨“ç·´ç¹¼çºŒ...\")\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        print(f\"åœ¨ {output_dir} æ²’æœ‰æ‰¾åˆ°æª¢æŸ¥é»ï¼Œå¾é ­é–‹å§‹è¨“ç·´...\")\n",
    "        trainer.train()\n",
    "\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    plot_losses(cb.logs, f\"{model_type} Training & Validation Loss\", f\"results/{model_type.lower()}_loss_02.png\")\n",
    "\n",
    "    # åˆä½µ LoRA â†’ å–®ä¸€ LLMï¼ˆç´”æ–‡å­—ï¼‰\n",
    "    merged = trainer.model.merge_and_unload()\n",
    "    merged_dir = os.path.join(output_dir, \"final_02\")\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    merged.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    print(f\"è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹å·²ä¿å­˜æ–¼ {merged_dir}\")\n",
    "\n",
    "    # é‡‹æ”¾\n",
    "    del trainer, peft_model, merged\n",
    "    torch.cuda.empty_cache()\n",
    "    return merged_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02533151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== é–‹å§‹è¨“ç·´ CoT æ¨¡å‹ï¼ˆ_02ï¼‰ ===\n",
      "trainable params: 24,313,856 || all params: 3,631,066,112 || trainable%: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åœ¨ ./breeze2-lora-cot-02 æ‰¾åˆ°æª¢æŸ¥é»ï¼Œå¾ä¸Šä¸€æ¬¡è¨“ç·´ç¹¼çºŒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\transformers\\trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/4480 [00:00<?, ?it/s]\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\peft\\utils\\save_and_load.py:218: UserWarning: Could not find a config file in /mnt/shared/tp1-user/training/outputs/BreezeTinyInstruct_v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.2485, 'train_samples_per_second': 288712.248, 'train_steps_per_second': 18029.424, 'train_loss': 0.0, 'epoch': 19.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_28524\\1138968316.py:27: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend(); plt.grid(True); plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss åœ–è¡¨å·²ä¿å­˜åˆ° results/cot_loss_02.png\n",
      "è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹å·²ä¿å­˜æ–¼ ./breeze2-lora-cot-02\\final_02\n",
      "\n",
      "=== é–‹å§‹è¨“ç·´ AO æ¨¡å‹ï¼ˆ_02ï¼‰ ===\n",
      "trainable params: 24,313,856 || all params: 3,631,066,112 || trainable%: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åœ¨ ./breeze2-lora-ao-02 æ‰¾åˆ°æª¢æŸ¥é»ï¼Œå¾ä¸Šä¸€æ¬¡è¨“ç·´ç¹¼çºŒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\transformers\\trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/1160 [00:00<?, ?it/s]\n",
      "c:\\Users\\user\\Desktop\\0801\\.venv\\lib\\site-packages\\peft\\utils\\save_and_load.py:218: UserWarning: Could not find a config file in /mnt/shared/tp1-user/training/outputs/BreezeTinyInstruct_v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.4075, 'train_samples_per_second': 45982.427, 'train_steps_per_second': 2846.298, 'train_loss': 0.0, 'epoch': 19.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_28524\\1138968316.py:27: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend(); plt.grid(True); plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss åœ–è¡¨å·²ä¿å­˜åˆ° results/ao_loss_02.png\n",
      "è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹å·²ä¿å­˜æ–¼ ./breeze2-lora-ao-02\\final_02\n",
      "å…©å€‹æ¨¡å‹çš†å·²å®Œæˆè¨“ç·´ä¸¦åˆä½µå­˜æª”ã€‚\n"
     ]
    }
   ],
   "source": [
    "# cell 7 â€” åŸ·è¡Œè¨“ç·´ï¼ˆCoT / AOï¼‰\n",
    "\n",
    "merged_dir_cot_02 = train_and_save_model(cot_train_data_file, val_data_file, OUTPUT_DIR_COT, \"CoT\")\n",
    "merged_dir_ao_02  = train_and_save_model(ao_train_data_file,  val_data_file, OUTPUT_DIR_AO,  \"AO\")\n",
    "print(\"å®Œæˆè¨“ç·´ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è¼‰å…¥ä¸¦è©•ä¼°åŸç”Ÿæ¨¡å‹ï¼ˆä½¿ç”¨åŸå§‹æç¤ºï¼‰===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.64s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹ä¿å­˜ base æ¨¡å‹è¼¸å‡ºåˆ° results/base_model_outputs_02.jsonl ...\n",
      "å®Œæˆï¼è¼¸å‡ºå·²ä¿å­˜åˆ° results/base_model_outputs_02.jsonl\n",
      "\n",
      "=== è¼‰å…¥ä¸¦è©•ä¼° CoT æ¨¡å‹ï¼ˆä½¿ç”¨ COT æç¤ºï¼‰===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹ä¿å­˜ cot æ¨¡å‹è¼¸å‡ºåˆ° results/ft_cot_outputs_02.jsonl ...\n",
      "å®Œæˆï¼è¼¸å‡ºå·²ä¿å­˜åˆ° results/ft_cot_outputs_02.jsonl\n",
      "\n",
      "=== è¼‰å…¥ä¸¦è©•ä¼° AO æ¨¡å‹ï¼ˆä½¿ç”¨ AO æç¤ºï¼‰===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹ä¿å­˜ ao æ¨¡å‹è¼¸å‡ºåˆ° results/ft_ao_outputs_02.jsonl ...\n",
      "å®Œæˆï¼è¼¸å‡ºå·²ä¿å­˜åˆ° results/ft_ao_outputs_02.jsonl\n",
      "\n",
      "æ‰€æœ‰æ¨¡å‹çš„è©•ä¼°å·²å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# cell 8 â€” è¼‰å…¥åˆä½µå¾Œæ¨¡å‹ä¸¦è©•ä¼° / è¼¸å‡ºï¼ˆ_02ï¼‰\n",
    "\n",
    "# å¼•å…¥å¿…è¦çš„å‡½å¼åº«\n",
    "import os, json, gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# æ¨è«–è¨­å®š\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True, temperature=0.01, top_p=0.01,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "def generate_prompt(question, model_type):\n",
    "    if model_type == \"base\":\n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    system_p = system_prompt_cot if model_type == \"cot\" else system_prompt_ao\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_p},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(model.device)\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    output_str = tokenizer.decode(outputs[0])\n",
    "    return output_str.replace(prompt, \"\")\n",
    "\n",
    "test_dataset = load_dataset(\"json\", data_files={\"test\": test_data_file}, split=\"test\")\n",
    "\n",
    "def save_outputs(model, tokenizer, dataset, file_path, model_type):\n",
    "    print(f\"é–‹å§‹ä¿å­˜ {model_type} æ¨¡å‹è¼¸å‡ºåˆ° {file_path} ...\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in dataset:\n",
    "            user_message = ex[\"messages\"][1][\"content\"]\n",
    "            prompt = generate_prompt(user_message, model_type)\n",
    "            raw_output = infer(model, tokenizer, prompt)\n",
    "            rec = {\n",
    "                \"question\": user_message,\n",
    "                \"raw_output\": raw_output,\n",
    "                \"target\": ex[\"messages\"][2][\"content\"],\n",
    "            }\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"å®Œæˆï¼è¼¸å‡ºå·²ä¿å­˜åˆ° {file_path}\")\n",
    "\n",
    "# è¨­å®šè¼¸å‡ºæª”æ¡ˆè·¯å¾‘\n",
    "BASE_MODEL_OUTPUTS = \"results/base_model_outputs_02.jsonl\"\n",
    "FT_COT_OUTPUTS = \"results/ft_cot_outputs_02.jsonl\"\n",
    "FT_AO_OUTPUTS  = \"results/ft_ao_outputs_02.jsonl\"\n",
    "\n",
    "# 1) è¼‰å…¥ä¸¦è©•ä¼°åŸç”Ÿæ¨¡å‹ \n",
    "print(\"=== è¼‰å…¥ä¸¦è©•ä¼°åŸç”Ÿæ¨¡å‹ï¼ˆä½¿ç”¨åŸå§‹æç¤ºï¼‰===\")\n",
    "try:\n",
    "    wrap_base = AutoModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        img_context_token_id=128212,\n",
    "    ).eval()\n",
    "    if hasattr(wrap_base, \"language_model\"):\n",
    "        base_model_for_eval = wrap_base.language_model\n",
    "    else:\n",
    "        for cand in (\"llm\", \"text_model\", \"lm\", \"model\"):\n",
    "            if hasattr(wrap_base, cand):\n",
    "                base_model_for_eval = getattr(wrap_base, cand)\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError(\"æ‰¾ä¸åˆ°æ–‡å­— LLM å­æ¨¡çµ„ã€‚\")\n",
    "    base_model_for_eval.to(DEVICE).eval()\n",
    "    save_outputs(base_model_for_eval, tokenizer, test_dataset, BASE_MODEL_OUTPUTS, \"base\")\n",
    "except Exception as e:\n",
    "    print(f\"åŸç”Ÿæ¨¡å‹è¼‰å…¥æˆ–è©•ä¼°å¤±æ•—ï¼ŒéŒ¯èª¤ï¼š{e}\")\n",
    "finally:\n",
    "    try:\n",
    "        del base_model_for_eval, wrap_base\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# 2) è¼‰å…¥ä¸¦è©•ä¼° CoT å¾®èª¿å¾Œæ¨¡å‹ \n",
    "print(\"\\n=== è¼‰å…¥ä¸¦è©•ä¼° CoT æ¨¡å‹ï¼ˆä½¿ç”¨ COT æç¤ºï¼‰===\")\n",
    "try:\n",
    "    merged_model_cot = AutoModelForCausalLM.from_pretrained(\n",
    "        os.path.join(OUTPUT_DIR_COT, \"final_02\"),\n",
    "        torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    ).eval()\n",
    "    save_outputs(merged_model_cot, tokenizer, test_dataset, FT_COT_OUTPUTS, \"cot\")\n",
    "finally:\n",
    "    try:\n",
    "        del merged_model_cot\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# 3) è¼‰å…¥ä¸¦è©•ä¼° AO å¾®èª¿å¾Œæ¨¡å‹ \n",
    "print(\"\\n=== è¼‰å…¥ä¸¦è©•ä¼° AO æ¨¡å‹ï¼ˆä½¿ç”¨ AO æç¤ºï¼‰===\")\n",
    "try:\n",
    "    merged_model_ao = AutoModelForCausalLM.from_pretrained(\n",
    "        os.path.join(OUTPUT_DIR_AO, \"final_02\"),\n",
    "        torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    ).eval()\n",
    "    save_outputs(merged_model_ao, tokenizer, test_dataset, FT_AO_OUTPUTS, \"ao\")\n",
    "finally:\n",
    "    try:\n",
    "        del merged_model_ao\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "print(\"\\næ‰€æœ‰æ¨¡å‹çš„è©•ä¼°å·²å®Œæˆã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
